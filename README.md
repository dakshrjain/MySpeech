# MySpeech
A Web Conferencing System for Speech and hearing impaired Using ML

Abstract

We depend on verbal communication and body language for our daily life communication. The speech and hearing-impaired people can only on gestures to communicate. Gestures are defined as the motion of the body that is intended to communicate with other agents. For a successful communication sender and the receiver must have the same set of information for a particular gesture. As per the context of this project, gesture is defined as an expressive movement of hands which has a particular message, to be communicated precisely between a sender and a receiver. Hand gestures can be classified in two categories: static and dynamic. A static hand gesture is a particular hand configuration and pose, represented by a single image. A dynamic hand gesture is a moving gesture, which is represented by a sequence of images This project focuses on the recognition of static images. American Sign Language (ASL) is a natural language which is a predominant sign language of speech and hearing impaired people in the United States and most of Canada. Besides America, dialects of ASL are used in many countries around the world, including much of West Africa and parts of Southeast Asia. That is why, a fast and accurate system is required to make this human computer interaction (HCI) more convenient to the deaf and dumb people.

Introduction

World Health Organization (WHO) survey states that above 6% of the world’s population is suffering from hearing impairment these people use sign languages to communicate that are visual representation of thoughts through hand gestures, facial expressions and body movements which is difficult to understand by verbal speakers as sign language is completely independent language from its counterpart of verbal language. Also, for sign languages they have their own grammar and syntax. There are different types of sign language based on location and language status. For example, American Sign Language, Bangla Sign Language, Indian Sign Language, etc. Also, the same gesture can be interpreted differently depending on the variations used by the user in the same sign language. Sometimes one gesture represents the whole word at a time can represent only one alphabet or number, sometimes additionsometimes a combination.

There are three types of sign language. Are as follow:

• Non-manual features: Tongue, facial expression, body pose, and hand gesture - all of them are used to communicate.

• Word level sign spelling: Each gesture presents a whole word.

• Finger vocabulary: One gesture represents the alphabet/numbers.

Problem Statement

To design a real time system for speech/hearing impaired that provides a better way in meetings.

Objectives

• To design a system for speech/hearing impaired that provides a better way to confer in public meetings

• To implement a plan to reduce the communication gap between two types of people

• To enable hassle free communications even in online meetings

Software Requirements

• Django and related dependencies

• Flask and related dependencies

• Firebase Database and related dependencies

• CV2

• Numpy

• Pickle

• Mediapipe

• Html & CSS

• Javascript

• Sklearn Libraries

Hardware Requirements

• Web Cam (Optional)

• Internet Connection

• Windows 8 and Above

Conclusion

In conclusion, developing a web conferencing sign language recognition system using MediaPipe and CNN has the potential to significantly improve the accessibility of web conferencing for the deaf and hard of hearing community. The use of MediaPipe and CNN technologies allows for real-time, accurate recognition of sign language gestures, enabling deaf and hard of hearing individuals to effectively communicate and participate in online meetings. However, the development of such a system requires a significant number of resources, including a team of experienced developers and engineers, access to high-quality training datasets, and the necessary hardware and software infrastructure.

The project timeline can also vary depending on the project scope, team size, and availability of resources. Despite these challenges, the development of a web conferencing sign language recognition system can have a positive impact on the lives of deaf and hard of hearing individuals, allowing them to fully participate in online meetings and bridge the communication gap with hearing individuals.

Future Scope

The future scope for web conferencing sign language recognition using MediaPipe and CNN is vast and promising. Here are some potential future developments for this technology: • Enhanced accuracy: While the current MediaPipe and CNN models have shown high accuracy rates in recognizing sign language gestures, there is always room for improvement. Future developments could focus on refining the algorithms and models to improve accuracy and reduce false positives. • Integration with virtual and augmented reality: With the growing popularity of virtual and augmented reality technologies, there is potential for web conferencing sign language recognition to be integrated into these environments, allowing for more immersive and interactive communication experiences. • Multi-language support: The current system primarily focuses on American Sign Language (ASL), but there is potential for the technology to support other sign languages around the world. This could open up the use of web conferencing sign language recognition to a wider global audience. • Integration with other assistive technologies: Web conferencing sign language recognition could be integrated with other assistive technologies, such as text-to-speech or speech-to-text software, to create a more inclusive and accessible communication platform for people with different communication needs. • Mobile device support: With the increasing use of mobile devices for web conferencing, future developments could focus on optimizing the technology for use on smartphones and tablets, allowing for greater accessibility and convenience for users.

Overall, the future scope for web conferencing sign language recognition using Mediapipe and CNN is promising, with potential for improved accuracy, wider language support, and integration with other technologies to create a more inclusive and accessible communication platform for all.
